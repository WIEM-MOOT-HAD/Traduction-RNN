{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T11:04:10.818248Z",
     "start_time": "2022-12-15T11:04:10.808256Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manipulate data \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#data vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "\n",
    "# pre traitment & model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:11.659477Z",
     "start_time": "2022-12-15T10:43:11.213380Z"
    }
   },
   "outputs": [],
   "source": [
    "french_sentences = open(\"./data/small_vocab_fr.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "english_sentences = open(\"./data/small_vocab_en.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "data_fr = pd.read_csv('./data/small_vocab_fr.txt', sep = \"\\t\", header = None)\n",
    "data_en = pd.read_csv('./data/small_vocab_en.txt', sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenisation \n",
    "Pour qu'un réseau neuronal puisse prédire sur des données textuelles, celles-ci doivent d'abord être transformées en données qu'il peut comprendre. Des données textuelles comme \" chien \" sont une séquence d'encodages de caractères ASCII. Comme un réseau neuronal est une série d'opérations de multiplication et d'addition, les données d'entrée doivent être des nombres.\n",
    "\n",
    "Nous pouvons transformer chaque caractère en un nombre ou chaque mot en un nombre. On parle alors d'identifiants de caractères et de mots, respectivement. Les identifiants des caractères sont utilisés pour les modèles de niveau caractère qui génèrent des prédictions de texte pour chaque caractère. Un modèle au niveau du mot utilise des identifiants de mot qui génèrent des prédictions de texte pour chaque mot. Les modèles au niveau des mots ont tendance à mieux apprendre, car ils sont moins complexes, nous les utiliserons donc.\n",
    "\n",
    "Transformez chaque phrase en une séquence d'identifiants de mots en utilisant la fonction Tokenizer de Keras. Utilisez cette fonction pour tokeniser les phrases anglaises et françaises dans la cellule ci-dessous.\n",
    "\n",
    "L'exécution de la cellule lancera tokenize sur des données d'échantillon et montrera la sortie pour le débogage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:11.675097Z",
     "start_time": "2022-12-15T10:43:11.663539Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog .', 'By Jove , my quick study of lexicography won a prize .', 'This is a short sentence .']\n",
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding \n",
    "Lorsque l'on regroupe les séquences d'identifiants de mots, chaque séquence doit être de la même longueur. Comme la longueur des phrases est dynamique, nous pouvons ajouter un remplissage à la fin des séquences pour qu'elles aient la même longueur.\n",
    "\n",
    "Assurez-vous que toutes les séquences anglaises ont la même longueur et que toutes les séquences françaises ont la même longueur en ajoutant du remplissage à la fin de chaque séquence en utilisant la fonction pad_sequences de Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:11.690135Z",
     "start_time": "2022-12-15T10:43:11.677515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre processing pipeline\n",
    "\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline. Instead, we've provided you with the implementation of the preprocess function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:18.655507Z",
     "start_time": "2022-12-15T10:43:11.694505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:18.671092Z",
     "start_time": "2022-12-15T10:43:18.658718Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving tokenizer\n",
    "with open('french_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# english_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:18.686463Z",
     "start_time": "2022-12-15T10:43:18.673328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x0000028FEA1BF100>\n"
     ]
    }
   ],
   "source": [
    "# loading tokenizer\n",
    "with open('english_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "De l'identité au texte\n",
    "Le réseau neuronal va traduire l'entrée en mots ids, ce qui n'est pas la forme finale que nous voulons. Nous voulons la traduction française. La fonction logits_to_text fait le lien entre les logits du réseau neuronal et la traduction française. Vous utiliserez cette fonction pour mieux comprendre la sortie du réseau neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:18.702291Z",
     "start_time": "2022-12-15T10:43:18.689465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = ''\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T11:10:07.926317Z",
     "start_time": "2022-12-15T11:10:07.911002Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "#     # Build the layers    \n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Embedding\n",
    "#     model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "#                          input_shape=input_shape[1:]))\n",
    "#     # Encoder\n",
    "#     model.add(Bidirectional(GRU(128)))\n",
    "#     model.add(RepeatVector(output_sequence_length))\n",
    "#     # Decoder\n",
    "#     model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "#     model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    # Add an Embedding layer expecting input vocab of size 1000, and\n",
    "    # output embedding dimension of size 64.\n",
    "    model.add(Embedding(english_vocab_size, \n",
    "                                  128, \n",
    "                                  input_length=input_shape[1],\n",
    "                                  input_shape=input_shape[1:]))\n",
    "\n",
    "    # Add a LSTM layer with 128 internal units.\n",
    "    model.add(LSTM(128))\n",
    "\n",
    "    # Add a Dense layer with 10 units.\n",
    "#     model.add(Dense(21))\n",
    "# repeatvector repeats the input for the desired number of times to change 2D-array to 3D array\n",
    "    model.add(RepeatVector(max_french_sequence_length))\n",
    "    model.add(LSTM(256, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T11:10:11.224084Z",
     "start_time": "2022-12-15T11:10:10.749814Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 15, 128)           25600     \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  (None, 21, 128)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 21, 256)           394240    \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 21, 345)          88665     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,089\n",
      "Trainable params: 640,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model = model_final(preproc_english_sentences.shape,\n",
    "                    preproc_french_sentences.shape[1],\n",
    "                    len(english_tokenizer.word_index)+1,\n",
    "                    len(french_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T11:10:52.866682Z",
     "start_time": "2022-12-15T11:10:52.853682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137861, 15), (137861, 21, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_english_sentences.shape, preproc_french_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T11:10:15.348980Z",
     "start_time": "2022-12-15T11:10:15.022821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 21, 1) and (None, 21, 345) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreproc_english_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreproc_french_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m520\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexr1cesxx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 21, 1) and (None, 21, 345) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(preproc_english_sentences, preproc_french_sentences, batch_size=520, epochs=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.038054Z",
     "start_time": "2022-12-15T10:43:19.038054Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot accuracy & loss metrics\n",
    "def plot_metrics(metrics, history):\n",
    "    loss_train = history.history[metrics]\n",
    "    loss_val = history.history['val_'+metrics]\n",
    "    epochs = range(1,26)\n",
    "    label_train = 'Training' + metrics\n",
    "    label_val = 'Validation' + metrics\n",
    "    plt.plot(epochs, loss_train, 'g', label=label_train)\n",
    "    plt.plot(epochs, loss_val, 'b', label=label_val)\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.040056Z",
     "start_time": "2022-12-15T10:43:19.040056Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics('accuracy', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.042056Z",
     "start_time": "2022-12-15T10:43:19.042056Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics('loss', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.044056Z",
     "start_time": "2022-12-15T10:43:19.044056Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_predict():\n",
    "    \n",
    "    y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "    y_id_to_word[0] = ''\n",
    "\n",
    "    sentence = 'india is relaxing during may but it is usually busy in winter'\n",
    "    \n",
    "    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    \n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "    \n",
    "    sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
    "    \n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.047056Z",
     "start_time": "2022-12-15T10:43:19.047056Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.048056Z",
     "start_time": "2022-12-15T10:43:19.048056Z"
    }
   },
   "outputs": [],
   "source": [
    "fdfddfdfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.050056Z",
     "start_time": "2022-12-15T10:43:19.050056Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "y_id_to_word[0] = ''\n",
    "\n",
    "sentence = 'new jersey is sometines beautiful'\n",
    "sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "sentences = np.array([sentence[0], x[0]])\n",
    "predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "print('Sample 1:')\n",
    "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "print('Sample 2:')\n",
    "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.052059Z",
     "start_time": "2022-12-15T10:43:19.052059Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    print(x.shape, y.shape[1], len(x_tk.word_index)+1, len(y_tk.word_index)+1)\n",
    "    \n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    history = model.fit(x, y, batch_size=520, epochs=25, validation_split=0.2)\n",
    "    model.save(\"model.h5\")\n",
    "    # DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = ''\n",
    "\n",
    "    sentence = 'new jersey is sometines beautiful'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.054060Z",
     "start_time": "2022-12-15T10:43:19.054060Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.056057Z",
     "start_time": "2022-12-15T10:43:19.056057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "\n",
    "def preprocess_predict():\n",
    "    \n",
    "    y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "    y_id_to_word[0] = ''\n",
    "\n",
    "    sentence = 'india is relaxing during may but it is usually busy in winter'\n",
    "    \n",
    "    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    \n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "    \n",
    "    sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
    "    \n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "#     return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.058060Z",
     "start_time": "2022-12-15T10:43:19.058060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "res = preprocess_predict()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.060058Z",
     "start_time": "2022-12-15T10:43:19.060058Z"
    }
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualise accuracy & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.062069Z",
     "start_time": "2022-12-15T10:43:19.062069Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,26)b\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.064461Z",
     "start_time": "2022-12-15T10:43:19.064461Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_train = history.history['accuracy']\n",
    "loss_val = history.history['val_accuracy']\n",
    "epochs = range(1,26)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-15T10:43:19.066474Z",
     "start_time": "2022-12-15T10:43:19.066474Z"
    }
   },
   "outputs": [],
   "source": [
    "history.save('model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
