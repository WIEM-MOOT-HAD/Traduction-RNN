{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies if not installed\n",
    "#!pip install --upgrade tensorflow-gpu=2.3.0\n",
    "\n",
    "#!pip install nltk\n",
    "\n",
    "#!pip install gensim\n",
    "\n",
    "#!pip install spacy\n",
    "\n",
    "#!pip install plotly\n",
    "\n",
    "#!pip install pandas\n",
    "\n",
    "#!pip install numpy\n",
    "\n",
    "#!pip install matplotlib\n",
    "\n",
    "#!pip install seaborn\n",
    "\n",
    "#!pip install wordcloud\n",
    "\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will import all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, LSTM, Embedding, RepeatVector\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "filename_en = './data/small_vocab_en.txt'\n",
    "filename_fr = './data/small_vocab_fr.txt'\n",
    "df_english = pd.read_csv(filename_en, sep='/t', names=['english'])\n",
    "df_french = pd.read_csv(filename_fr, sep='/t', names=['french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey is sometimes quiet during autumn , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the united states is usually chilly during jul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california is usually quiet during march , and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the united states is sometimes mild during jun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your least liked fruit is the grape , but my l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137855</th>\n",
       "      <td>france is never busy during march , and it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137856</th>\n",
       "      <td>india is sometimes beautiful during spring , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137857</th>\n",
       "      <td>india is never wet during summer , but it is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137858</th>\n",
       "      <td>france is never chilly during january , but it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137859</th>\n",
       "      <td>the orange is her favorite fruit , but the ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137860 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english\n",
       "0       new jersey is sometimes quiet during autumn , ...\n",
       "1       the united states is usually chilly during jul...\n",
       "2       california is usually quiet during march , and...\n",
       "3       the united states is sometimes mild during jun...\n",
       "4       your least liked fruit is the grape , but my l...\n",
       "...                                                   ...\n",
       "137855  france is never busy during march , and it is ...\n",
       "137856  india is sometimes beautiful during spring , a...\n",
       "137857  india is never wet during summer , but it is s...\n",
       "137858  france is never chilly during january , but it...\n",
       "137859  the orange is her favorite fruit , but the ban...\n",
       "\n",
       "[137860 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize english data\n",
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey est parfois calme pendant l' automn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>les états-unis est généralement froid en juill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california est généralement calme en mars , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>les états-unis est parfois légère en juin , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>votre moins aimé fruit est le raisin , mais mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137855</th>\n",
       "      <td>la france est jamais occupée en mars , et il e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137856</th>\n",
       "      <td>l' inde est parfois belle au printemps , et il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137857</th>\n",
       "      <td>l' inde est jamais mouillé pendant l' été , ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137858</th>\n",
       "      <td>la france est jamais froid en janvier , mais i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137859</th>\n",
       "      <td>l'orange est son fruit préféré , mais la banan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137860 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   french\n",
       "0       new jersey est parfois calme pendant l' automn...\n",
       "1       les états-unis est généralement froid en juill...\n",
       "2       california est généralement calme en mars , et...\n",
       "3       les états-unis est parfois légère en juin , et...\n",
       "4       votre moins aimé fruit est le raisin , mais mo...\n",
       "...                                                   ...\n",
       "137855  la france est jamais occupée en mars , et il e...\n",
       "137856  l' inde est parfois belle au printemps , et il...\n",
       "137857  l' inde est jamais mouillé pendant l' été , ma...\n",
       "137858  la france est jamais froid en janvier , mais i...\n",
       "137859  l'orange est son fruit préféré , mais la banan...\n",
       "\n",
       "[137860 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize french data\n",
    "df_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english    0\n",
      "dtype: int64\n",
      "french    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there is any NULL data\n",
    "print(df_english.isnull().sum())\n",
    "print(df_french.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey is sometimes quiet during autumn , ...</td>\n",
       "      <td>new jersey est parfois calme pendant l' automn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the united states is usually chilly during jul...</td>\n",
       "      <td>les états-unis est généralement froid en juill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california is usually quiet during march , and...</td>\n",
       "      <td>california est généralement calme en mars , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the united states is sometimes mild during jun...</td>\n",
       "      <td>les états-unis est parfois légère en juin , et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your least liked fruit is the grape , but my l...</td>\n",
       "      <td>votre moins aimé fruit est le raisin , mais mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137855</th>\n",
       "      <td>france is never busy during march , and it is ...</td>\n",
       "      <td>la france est jamais occupée en mars , et il e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137856</th>\n",
       "      <td>india is sometimes beautiful during spring , a...</td>\n",
       "      <td>l' inde est parfois belle au printemps , et il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137857</th>\n",
       "      <td>india is never wet during summer , but it is s...</td>\n",
       "      <td>l' inde est jamais mouillé pendant l' été , ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137858</th>\n",
       "      <td>france is never chilly during january , but it...</td>\n",
       "      <td>la france est jamais froid en janvier , mais i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137859</th>\n",
       "      <td>the orange is her favorite fruit , but the ban...</td>\n",
       "      <td>l'orange est son fruit préféré , mais la banan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0       new jersey is sometimes quiet during autumn , ...   \n",
       "1       the united states is usually chilly during jul...   \n",
       "2       california is usually quiet during march , and...   \n",
       "3       the united states is sometimes mild during jun...   \n",
       "4       your least liked fruit is the grape , but my l...   \n",
       "...                                                   ...   \n",
       "137855  france is never busy during march , and it is ...   \n",
       "137856  india is sometimes beautiful during spring , a...   \n",
       "137857  india is never wet during summer , but it is s...   \n",
       "137858  france is never chilly during january , but it...   \n",
       "137859  the orange is her favorite fruit , but the ban...   \n",
       "\n",
       "                                                   french  \n",
       "0       new jersey est parfois calme pendant l' automn...  \n",
       "1       les états-unis est généralement froid en juill...  \n",
       "2       california est généralement calme en mars , et...  \n",
       "3       les états-unis est parfois légère en juin , et...  \n",
       "4       votre moins aimé fruit est le raisin , mais mo...  \n",
       "...                                                   ...  \n",
       "137855  la france est jamais occupée en mars , et il e...  \n",
       "137856  l' inde est parfois belle au printemps , et il...  \n",
       "137857  l' inde est jamais mouillé pendant l' été , ma...  \n",
       "137858  la france est jamais froid en janvier , mais i...  \n",
       "137859  l'orange est son fruit préféré , mais la banan...  \n",
       "\n",
       "[137860 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat both side by side for easy viewing\n",
    "df = pd.concat([df_english, df_french], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(x):\n",
    "    return re.sub('[!#?,.:\";]', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['english'] = df['english'].apply(remove_punc)\n",
    "df['french'] = df['french'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique english words  199\n",
      "Number of unique english words  350\n"
     ]
    }
   ],
   "source": [
    "# To get number of unique words\n",
    "\n",
    "english_words = []\n",
    "french_words = []\n",
    "\n",
    "def get_unique(x, word_list):\n",
    "    for word in x.split():\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "        \n",
    "df['english'].apply(lambda x:get_unique(x,english_words))\n",
    "df['french'].apply(lambda x:get_unique(x,french_words))\n",
    "\n",
    "total_english_words = len(english_words)\n",
    "total_french_words = len(french_words)\n",
    "\n",
    "print(\"Number of unique english words \", total_english_words)\n",
    "print(\"Number of unique english words \", total_french_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in any english sentence is  15\n",
      "Max number of words in any french sentence is  23\n"
     ]
    }
   ],
   "source": [
    "# Find max number of words in sentence so we can later do padding\n",
    "\n",
    "maxlen_english = -1\n",
    "for doc in df[\"english\"]:\n",
    "    #print(doc)\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if maxlen_english < len(tokens):\n",
    "        maxlen_english = len(tokens)\n",
    "        \n",
    "print(\"Max number of words in any english sentence is \", maxlen_english)\n",
    "\n",
    "maxlen_french = -1\n",
    "for doc in df.french:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if maxlen_french < len(tokens):\n",
    "        maxlen_french = len(tokens)\n",
    "        \n",
    "print(\"Max number of words in any french sentence is \", maxlen_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"english\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenier to tokenize the words and create sequences of tokenized words\n",
    "def tokenize_and_pad(x,maxlen):\n",
    "    tokenizer = Tokenizer(char_level = False)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen, padding = 'post')\n",
    "    return tokenizer, sequences, padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer, x_sequences, x_padded = tokenize_and_pad(df.english, maxlen_english)\n",
    "y_tokenizer, y_sequences, y_padded = tokenize_and_pad(df.french,  maxlen_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x7f0d6d02e0f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized version for english document\n",
      " the orange is her favorite fruit  but the banana is your favorite  \n",
      " is :  [[ 5 84  1 32 11 13  6  5 87  1 29 11  0  0  0]]\n",
      "The tokenized version for french document\n",
      " l'orange est son fruit préféré  mais la banane est votre favori  \n",
      " is :  [[84  1 20 16 17  5  7 87  1 40 93  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Visualize example of a token\n",
    "print(\"The tokenized version for english document\\n\", df.english[-1:].item(),\"\\n is : \", x_padded[-1:])\n",
    "\n",
    "print(\"The tokenized version for french document\\n\", df.french[-1:].item(),\"\\n is : \", y_padded[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain text from padded\n",
    "def pad_to_text(padded, tokenizer):\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    temp = ' '.join([id_to_word[j] for j in padded])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new jersey est parfois calme pendant l' automne et il est neigeux en avril         \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_to_text(y_padded[0], y_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete English Vocab Size: 200\n",
      "Complete French Vocab Size: 351\n"
     ]
    }
   ],
   "source": [
    "# Total vocab size, since we added padding we add 1 to the total word count\n",
    "english_vocab_size = total_english_words + 1\n",
    "print(\"Complete English Vocab Size:\", english_vocab_size)\n",
    "\n",
    "french_vocab_size = total_french_words + 1\n",
    "print(\"Complete French Vocab Size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 256)           51200     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 23, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 23, 256)           525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 23, 351)          90207     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,192,031\n",
      "Trainable params: 1,192,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BUILDING THE MODEL\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(english_vocab_size, 256, input_length = maxlen_english, mask_zero = True))\n",
    "\n",
    "# Encoder\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# decoder\n",
    "# repeatvector repeats the input for the desired number of times to change 2D-array to 3D array\n",
    "model.add(RepeatVector(maxlen_french))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 23, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the shape of target from 2D to 3D\n",
    "y_train = np.expand_dims(y_train, axis = 2)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = np.expand_dims(x_train, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 15, 1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "873/873 [==============================] - 380s 421ms/step - loss: 1.5381 - accuracy: 0.6278 - val_loss: 0.9653 - val_accuracy: 0.7163\n",
      "Epoch 2/20\n",
      "873/873 [==============================] - 388s 444ms/step - loss: 0.7353 - accuracy: 0.7815 - val_loss: 0.5638 - val_accuracy: 0.8324\n",
      "Epoch 3/20\n",
      "302/873 [=========>....................] - ETA: 4:27 - loss: 0.5224 - accuracy: 0.8437"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#with tf.device('/gpu:0'):\n",
    "history = model.fit(x_train, y_train, batch_size=128, validation_split = 0.1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./models/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f0cd58c9d68>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7cf15b838afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,26)\n",
    "plt.plot(epochs, accuracy, 'g', label='accuracy')\n",
    "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make prediction\n",
    "def prediction(x, x_tokenizer = x_tokenizer, y_tokenizer = y_tokenizer):\n",
    "    predictions = model.predict(x)[0]\n",
    "    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English word - they are going to india next may        \n",
      "\n",
      "Original French word - ils vont en inde en mai prochain                \n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted French word - va va va va avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original English word - paris is quiet during summer and it is cold in spring    \n",
      "\n",
      "Original French word - paris est calme pendant l' été et il fait froid au printemps           \n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted French word - grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand grand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original English word - china is usually pleasant during january and it is usually mild in march  \n",
      "\n",
      "Original French word - chine est généralement agréable en janvier et il est généralement doux en mars          \n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted French word - porcelaine porcelaine porcelaine porcelaine proches proches proches proches proches proches proches proches proches proches proches proches proches proches proches proches proches proches proches\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original English word - the united states is wet during november and it is warm in june  \n",
      "\n",
      "Original French word - les états unis est humide au mois de novembre et il est chaud en juin        \n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted French word - blanc blanc blanc blanc blanc votre votre votre votre votre votre votre votre votre votre votre votre votre votre votre votre votre votre\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original English word - he dislikes pears mangoes and grapes         \n",
      "\n",
      "Original French word - il n'aime les poires les mangues et les raisins              \n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted French word - monde frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet frisquet\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us take 5 examples\n",
    "for i in range(5):\n",
    "    print('Original English word - {}\\n'.format(pad_to_text(x_test[i], x_tokenizer)))\n",
    "    print('Original French word - {}\\n'.format(pad_to_text(y_test[i], y_tokenizer)))\n",
    "    print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(x_test[i:i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights separately\n",
    "model.save_weights('Final_weights.h5')\n",
    "\n",
    "#Save the model (including weights)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load to model we saved elsewhere\n",
    "model1 = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict using the model we saved\n",
    "# function to make prediction\n",
    "def prediction(x, x_tokenizer = x_tokenizer, y_tokenizer = y_tokenizer):\n",
    "    predictions = model.predict(x)[0]\n",
    "    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted French word - va va va va avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons avons\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(x_test[0:1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = [\"the united states is sometimes quiet during autumn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_text(padded, tokenizer):\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    #temp = ' '.join([id_to_word[j] for j in padded])\n",
    "    return id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad1(x,maxlen=maxlen_english, df=df[\"english\"]):\n",
    "    tokenizer = Tokenizer(char_level = False)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen, padding = 'post')\n",
    "    return tokenizer, sequences, padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1, sequences1, padded1 = tokenize_and_pad1(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x7f0cb619f9b0>\n",
      "[[5, 20, 21, 1, 8, 67, 4, 39]]\n",
      "[[ 5 20 21  1  8 67  4 39  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1)\n",
    "print(sequences1)\n",
    "print(padded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_text(padded, tokenizer):\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    #temp = ' '.join([id_to_word[j] for j in padded])\n",
    "    return id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"the united states is sometimes quiet during autumn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted French word - calme calme elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle elle\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(padded1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = [\"new jersey is sometimes quiet during autumn  and it is snowy in april\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2, sequences2, padded2 = tokenize_and_pad1(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted French word - aux ses ses ses tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour tour\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(padded2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_prediction(x,maxlen=maxlen_english, df=df[\"english\"]):\n",
    "    tokenizer = Tokenizer(char_level = False)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen, padding = 'post')\n",
    "    predictions = model.predict(padded)[0]\n",
    "    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    pred= ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = [\"india is sometimes busy during august but it is wonderful in fall \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aller aller aller aller aller aller aller était était était était était était était était était était était était était était était était'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_prediction(sent1,maxlen=maxlen_english, df=df[\"english\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(maxlen_english, ), dtype='int32',)\n",
    "    encoder_embedding = Embedding(encoder_inputs, output_dim= 20)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(maxlen_english, ), dtype='int32',)\n",
    "    decoder_embedding = embed_layer(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e1694e39137c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_model_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-e6558ada68a5>\u001b[0m in \u001b[0;36mseq2seq_model_builder\u001b[0;34m(HIDDEN_DIM)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen_english\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mencoder_LSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlayout_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_layout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Inject the layout parameter after the invocation of __init__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/layers/core/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_shape\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             raise ValueError(\n\u001b[1;32m    133\u001b[0m                 \u001b[0;34m\"Both `input_dim` and `output_dim` should be positive, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         raise TypeError(\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;34m\"Keras symbolic inputs/outputs do not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;34m\"implement `__len__`. You may be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;34m\"trying to pass Keras symbolic inputs/outputs \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "model = seq2seq_model_builder(HIDDEN_DIM=300)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_NLP: \n",
    "- padded \n",
    "- tokenizer\n",
    "- sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting helper\n",
      "  Downloading helper-2.5.0.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: helper\n",
      "  Building wheel for helper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for helper: filename=helper-2.5.0-py2.py3-none-any.whl size=19166 sha256=197e621cc0a6f82b11dfdefc0c32295b2b5f278656d4731145ede8d500a78806\n",
      "  Stored in directory: /home/wiem/snap/jupyter/6/.cache/pip/wheels/41/b4/cf/a065acf9ad15025a24e687f5cfddecc4f22f28e0844156ce69\n",
      "Successfully built helper\n",
      "Installing collected packages: pyyaml, helper\n",
      "Successfully installed helper-2.5.0 pyyaml-6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1, sequences1, padded1 = tokenize_and_pad1(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137860, 15)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "    \n",
    "# TODO: Build the layers\n",
    "model = Sequential()\n",
    "model.add(GRU(256,return_sequences=True, input_shape=x_train.shape))\n",
    "model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_16 (GRU)                (None, 124074, 256)       209664    \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 124074, 1024)     263168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 124074, 1024)      0         \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 124074, 351)      359775    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 832,607\n",
      "Trainable params: 832,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 15)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 15)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 23, 1)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 296, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_33\" is incompatible with the layer: expected shape=(None, 124074, 15), found shape=(None, 15)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-c88ee8bbbbf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 296, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_33\" is incompatible with the layer: expected shape=(None, 124074, 15), found shape=(None, 15)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=128, validation_split = 0.1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"bidirectional_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 137860, 15, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-260b8a28bea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_english\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;34mf\"expected ndim={spec.ndim}, found ndim={ndim}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"bidirectional_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 137860, 15, 128)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "\n",
    "# Build the layers    \n",
    "model = Sequential()\n",
    "# Embedding\n",
    "model.add(Embedding(english_vocab_size, 128, input_length=maxlen_english, input_shape=x_padded.shape))\n",
    "# Encoder\n",
    "model.add(Bidirectional(GRU(128)))\n",
    "model.add(RepeatVector(output_sequence_length))\n",
    "# Decoder\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 15)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 23, 1)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 15, 256)           51200     \n",
      "                                                                 \n",
      " simple_rnn_12 (SimpleRNN)   (None, 128)               49280     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN = keras.Sequential()\n",
    "model_RNN.add(Embedding(english_vocab_size, 256, input_length = maxlen_english, mask_zero = True))\n",
    "model_RNN.add(layers.SimpleRNN(128))\n",
    "model_RNN.add(layers.Dense(10))\n",
    "#model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "model_RNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 23 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](remove_squeezable_dimensions/Squeeze)' with input shapes: [?,23].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-f803f96b6ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_RNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/home/wiem/snap/jupyter/common/lib/python3.7/site-packages/keras/utils/metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 23 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](remove_squeezable_dimensions/Squeeze)' with input shapes: [?,23].\n"
     ]
    }
   ],
   "source": [
    "history = model_RNN.fit(x_train_reshaped, y_train, batch_size=128, validation_split = 0.1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
